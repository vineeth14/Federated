{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import logging\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import syft as sy\n",
    "from syft.workers.websocket_client import WebsocketClientWorker\n",
    "from syft.workers.virtual import VirtualWorker\n",
    "from syft.frameworks.torch.federated import utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "LOG_INTERVAL = 25\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.conv1(x))\n",
    "        x = f.max_pool2d(x, 2, 2)\n",
    "        x = f.relu(self.conv2(x))\n",
    "        x = f.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return f.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def train_on_batches(worker, batches, model_in, device, lr):\n",
    "    \"\"\"Train the model on the worker on the provided batches\n",
    "    Args:\n",
    "        worker(syft.workers.BaseWorker): worker on which the\n",
    "        training will be executed\n",
    "        batches: batches of data of this worker\n",
    "        model_in: machine learning model, training will be done on a copy\n",
    "        device (torch.device): where to run the training\n",
    "        lr: learning rate of the training steps\n",
    "    Returns:\n",
    "        model, loss: obtained model and loss after training\n",
    "    \"\"\"\n",
    "    model = model_in.copy()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)  # TODO momentum is not supported at the moment\n",
    "\n",
    "    model.train()\n",
    "    model.send(worker)\n",
    "    loss_local = False\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(batches):\n",
    "        loss_local = False\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = f.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            loss = loss.get()  # <-- NEW: get the loss back\n",
    "            loss_local = True\n",
    "            logger.debug(\n",
    "                \"Train Worker {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    worker.id,\n",
    "                    batch_idx,\n",
    "                    len(batches),\n",
    "                    100.0 * batch_idx / len(batches),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    if not loss_local:\n",
    "        loss = loss.get()  # <-- NEW: get the loss back\n",
    "    model.get()  # <-- NEW: get the model back\n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def get_next_batches(fdataloader: sy.FederatedDataLoader, nr_batches: int):\n",
    "    \"\"\"retrieve next nr_batches of the federated data loader and group\n",
    "    the batches by worker\n",
    "    Args:\n",
    "        fdataloader (sy.FederatedDataLoader): federated data loader\n",
    "        over which the function will iterate\n",
    "        nr_batches (int): number of batches (per worker) to retrieve\n",
    "    Returns:\n",
    "        Dict[syft.workers.BaseWorker, List[batches]]\n",
    "    \"\"\"\n",
    "    batches = {}\n",
    "    for worker_id in fdataloader.workers:\n",
    "        worker = fdataloader.federated_dataset.datasets[worker_id].location\n",
    "        batches[worker] = []\n",
    "    try:\n",
    "        for i in range(nr_batches):\n",
    "            next_batches = next(fdataloader)\n",
    "            for worker in next_batches:\n",
    "                batches[worker].append(next_batches[worker])\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return batches\n",
    "\n",
    "\n",
    "def train(model, device, federated_train_loader, lr, federate_after_n_batches):\n",
    "    model.train()\n",
    "\n",
    "    nr_batches = federate_after_n_batches\n",
    "\n",
    "    models = {}\n",
    "    loss_values = {}\n",
    "\n",
    "    iter(federated_train_loader)  # initialize iterators\n",
    "    batches = get_next_batches(federated_train_loader, nr_batches)\n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        logger.debug(\n",
    "            \"Starting training round, batches [{}, {}]\".format(counter, counter + nr_batches)\n",
    "        )\n",
    "        data_for_all_workers = True\n",
    "        for worker in batches:\n",
    "            curr_batches = batches[worker]\n",
    "            if curr_batches:\n",
    "                models[worker], loss_values[worker] = train_on_batches(\n",
    "                    worker, curr_batches, model, device, lr\n",
    "                )\n",
    "            else:\n",
    "                data_for_all_workers = False\n",
    "        counter += nr_batches\n",
    "        if not data_for_all_workers:\n",
    "            logger.debug(\"At least one worker ran out of data, stopping.\")\n",
    "            break\n",
    "\n",
    "        model = utils.federated_avg(models)\n",
    "        batches = get_next_batches(federated_train_loader, nr_batches)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += f.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    logger.debug(\"\\n\")\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    logger.info(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def define_and_get_arguments(args=sys.argv[1:]):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Run federated learning using websocket client workers.\"\n",
    "    )\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"batch size of the training\")\n",
    "    parser.add_argument(\n",
    "        \"--test_batch_size\", type=int, default=1000, help=\"batch size used for the test data\"\n",
    "    )\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2, help=\"number of epochs to train\")\n",
    "    parser.add_argument(\n",
    "        \"--federate_after_n_batches\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"number of training steps performed on each remote worker \" \"before averaging\",\n",
    "    )\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate\")\n",
    "    parser.add_argument(\"--cuda\", action=\"store_true\", help=\"use cuda\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, help=\"seed used for randomization\")\n",
    "    parser.add_argument(\"--save_model\", action=\"store_true\", help=\"if set, model will be saved\")\n",
    "    parser.add_argument(\n",
    "        \"--verbose\",\n",
    "        \"-v\",\n",
    "        action=\"store_true\",\n",
    "        help=\"if set, websocket client workers will \" \"be started in verbose mode\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_virtual\", action=\"store_true\", help=\"if set, virtual workers will be used\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = define_and_get_arguments()\n",
    "\n",
    "    hook = sy.TorchHook(torch)\n",
    "\n",
    "    if args.use_virtual:\n",
    "        alice = VirtualWorker(id=\"alice\", hook=hook, verbose=args.verbose)\n",
    "        bob = VirtualWorker(id=\"bob\", hook=hook, verbose=args.verbose)\n",
    "        charlie = VirtualWorker(id=\"charlie\", hook=hook, verbose=args.verbose)\n",
    "    else:\n",
    "        kwargs_websocket = {\"host\": \"localhost\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "        alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket)\n",
    "        bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket)\n",
    "        charlie = WebsocketClientWorker(id=\"charlie\", port=8779, **kwargs_websocket)\n",
    "\n",
    "    workers = [alice, bob, charlie]\n",
    "\n",
    "    use_cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    federated_train_loader = sy.FederatedDataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"/data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ).federate(tuple(workers)),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        iter_per_worker=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"../data\",\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        logger.info(\"Starting epoch %s/%s\", epoch, args.epochs)\n",
    "        model = train(model, device, federated_train_loader, args.lr, args.federate_after_n_batches)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FORMAT = \"%(asctime)s %(levelname)s %(filename)s(l:%(lineno)d) - %(message)s\"\n",
    "    LOG_LEVEL = logging.DEBUG\n",
    "    logging.basicConfig(format=FORMAT, level=LOG_LEVEL)\n",
    "\n",
    "    websockets_logger = logging.getLogger(\"websockets\")\n",
    "    websockets_logger.setLevel(logging.DEBUG)\n",
    "    websockets_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
